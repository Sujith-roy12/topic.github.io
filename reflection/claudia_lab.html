<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Key Concepts in Algorithms</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        p {
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <h1>Key Concepts in Algorithms</h1>

    <h2>Time Complexity Analysis</h2>
    <p>Time complexity is all about figuring out how long an algorithm will take to finish, especially as the amount of data grows. It helps compare different ways of solving a problem to see which one is faster. Big-O notation is like a simple rating system that shows if something is quick, slow, or in-between. The goal is to find a solution that works well, even when the data gets really big. Time complexity measures how long an algorithm takes to run as the input size increases. Algorithms are categorized using Big-O notation: O(1) means it’s fast and takes the same time no matter the input size. O(log n) is very efficient, like binary search, where the problem size is halved with each step. O(n) means it processes each input once, like scanning through a list. O(n²) or worse, like O(2ⁿ), is slow, especially with large inputs. Understanding time complexity helps you pick the best approach to ensure a program runs efficiently, especially when handling millions of items.</p>

    <h2>Binary Search Tree (BST)</h2>
    <p>A Binary Search Tree is like an organized tree where smaller values go to the left, and larger ones go to the right. This makes it handy for finding things quickly. You can also add or remove items, but the tree needs to stay balanced to work fast. If it gets out of balance, it can slow down, so keeping it organized is key. A Binary Search Tree organizes data so smaller values go left and larger values go right. This structure lets you search, insert, or delete items efficiently in O(log n) time—provided the tree is balanced. In a balanced BST, the height is minimized, making operations fast. However, if the tree becomes unbalanced (like a linked list), operations degrade to O(n) time. Balanced BSTs like AVL or Red-Black Trees solve this problem by keeping the height small.</p>

    <h2>DFS (Depth-First Search) and BFS (Breadth-First Search)</h2>
    <p>DFS and BFS are two ways to explore data in graphs or trees. DFS goes as deep as possible down one path before backing up and trying another path. BFS, on the other hand, looks at everything nearby first before moving further out. DFS is good for solving puzzles or exploring all options, while BFS is great for finding the shortest path or checking things level by level. DFS explores one branch of a tree or graph as deeply as possible before backtracking, while BFS checks all neighbors level by level. DFS is efficient in terms of memory, as it uses a stack and takes O(V + E) time (V = vertices, E = edges). BFS uses a queue and also takes O(V + E) time but can require more memory, especially with wide graphs. DFS is great for tasks like checking if a path exists, while BFS is better for finding the shortest path in unweighted graphs.</p>

    <h2>Heap</h2>
    <p>A heap is a special kind of tree where the top (or root) always has the smallest or largest item. It’s useful when you need to quickly grab the most important thing, like in a to-do list where tasks have priorities. For example, heaps are used to sort data or manage tasks in order of importance, like scheduling jobs for a computer. A heap is a special tree where the root is either the smallest or largest value. Inserting or removing an item from a heap takes O(log n) time because it maintains order by reorganizing only part of the tree. Building a heap from scratch takes O(n) time. Heaps are commonly used in priority queues, where the highest-priority task is processed first, and in heapsort, which sorts data in O(n log n) time. They’re also used in algorithms like Dijkstra's to quickly find the next shortest path.</p>

    <h2>Sorting</h2>
    <p>Sorting is about putting things in order, like arranging numbers from smallest to largest or organizing names alphabetically. Some methods, like Bubble Sort, are easy to understand but slow. Others, like Quick Sort, are much faster but a bit trickier to learn. Sorting is used everywhere, from arranging search results to organizing files on your computer. Sorting organizes data into a specific order. Here’s a breakdown of common sorting algorithms and their efficiencies: Bubble Sort is simple but very slow, with a time complexity of O(n²). Quicksort is efficient on average with O(n log n) time but can degrade to O(n²) in the worst case if poorly implemented. Mergesort always takes O(n log n) time and works well for large datasets but needs extra memory. Heapsort is efficient with O(n log n) time and works in-place, meaning no extra space is needed. Sorting is used in search engines, file systems, and data analysis.</p>

    <h2>Pattern Searching</h2>
    <p>Pattern searching is finding one thing inside another, like looking for a word in a book or a phrase in a document. A basic way is to check every possible spot, but smarter methods like KMP and Rabin-Karp skip unnecessary checks to save time. It’s used in things like text editors, search engines, and even spam filters. Pattern searching finds one sequence of characters inside another, like finding a word in a sentence. The naive method checks every possible position and takes O(n * m) time, where n is the text length and m is the pattern length. KMP (Knuth-Morris-Pratt) optimizes by reusing previous match results, reducing the time to O(n + m). Rabin-Karp uses hashing to compare patterns, with average-case efficiency of O(n + m) but a worst case of O(n * m) due to hash collisions. Pattern searching is critical in applications like search engines, DNA sequence analysis, and spam detection.</p>

    <h2>Graph Algorithms</h2>
    <p>Graphs are like maps made of points (called nodes) connected by lines (called edges). Graph algorithms help solve problems like finding the shortest path between two points (like in GPS apps) or connecting everything as cheaply as possible (like laying cables in a city). They are used in things like social networks, delivery routes, and even games. Graphs are made of nodes (vertices) connected by lines (edges). Graph algorithms solve problems like finding the shortest path, minimum connections, or detecting cycles. Dijkstra’s Algorithm finds the shortest path in weighted graphs, taking O(V²) with a simple implementation or O((V + E) log V) with a priority queue (heap). Kruskal’s and Prim’s Algorithms find the minimum spanning tree, both taking O(E log E) time. Floyd-Warshall solves all-pairs shortest paths in O(V³) time, but it’s slow for large graphs. These algorithms are used in navigation apps, social networks, and network design.</p>
</body>
</html>
